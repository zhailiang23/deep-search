---
id: 006
title: JSON数据管理系统
epic: 创建智能搜索平台
status: pending
priority: medium
assignee: ""
labels: ["数据管理", "JSON处理", "文件上传", "数据验证", "批量导入"]
created: 2025-09-20T02:23:58Z
updated: 2025-09-20T02:45:32Z
estimate: M
parallel: false
dependencies: [005]
---

# JSON数据管理系统

## 描述

实现智能搜索平台的JSON数据管理系统，提供文件上传、动态字段映射、数据验证、批量导入功能。支持大规模JSON数据的处理和管理，确保数据质量和系统性能，为搜索功能提供可靠的数据源。

## 验收标准

- [ ] 文件上传响应时间 < 500ms (100MB以内文件)
- [ ] 批量导入处理速度 > 1000条/秒
- [ ] 数据验证准确率 100% (语法和业务规则)
- [ ] 动态字段映射配置生效时间 < 100ms
- [ ] 支持并发50个用户同时上传
- [ ] 数据导入错误率 < 0.1%
- [ ] 大文件(>1GB)分片上传成功率 > 99%
- [ ] 字段映射推荐准确率 > 85%
- [ ] 数据预览响应时间 < 200ms
- [ ] 系统资源占用 < 80% (CPU和内存)

## 技术细节

### 文件上传系统

**多种上传方式支持**
```java
@RestController
@RequestMapping("/api/data")
public class DataUploadController {

    @Autowired
    private FileUploadService fileUploadService;

    @Autowired
    private DataProcessingService dataProcessingService;

    @PostMapping("/upload")
    public ResponseEntity<UploadResponse> uploadFile(
            @RequestParam("file") MultipartFile file,
            @RequestParam(value = "chunkIndex", required = false) Integer chunkIndex,
            @RequestParam(value = "totalChunks", required = false) Integer totalChunks,
            @RequestParam(value = "uploadId", required = false) String uploadId,
            HttpServletRequest request) {

        try {
            UploadContext context = buildUploadContext(file, chunkIndex, totalChunks, uploadId, request);

            if (isChunkedUpload(chunkIndex, totalChunks)) {
                return handleChunkedUpload(context);
            } else {
                return handleDirectUpload(context);
            }
        } catch (Exception e) {
            logger.error("File upload failed", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                .body(UploadResponse.error("Upload failed: " + e.getMessage()));
        }
    }

    @PostMapping("/upload/url")
    public ResponseEntity<UploadResponse> uploadFromUrl(@RequestBody UrlUploadRequest request) {
        try {
            // 异步URL下载和处理
            String taskId = fileUploadService.uploadFromUrl(request.getUrl(), getCurrentUser());

            return ResponseEntity.ok(UploadResponse.builder()
                .taskId(taskId)
                .status("PROCESSING")
                .message("URL upload started")
                .build());
        } catch (Exception e) {
            return ResponseEntity.badRequest()
                .body(UploadResponse.error("URL upload failed: " + e.getMessage()));
        }
    }

    private ResponseEntity<UploadResponse> handleChunkedUpload(UploadContext context) {
        ChunkUploadResult result = fileUploadService.uploadChunk(context);

        if (result.isComplete()) {
            // 所有分片上传完成，开始合并
            String fileId = fileUploadService.mergeChunks(context.getUploadId());

            // 异步启动数据处理
            dataProcessingService.processFileAsync(fileId, getCurrentUser());

            return ResponseEntity.ok(UploadResponse.builder()
                .fileId(fileId)
                .status("UPLOADED")
                .message("File uploaded successfully")
                .build());
        } else {
            return ResponseEntity.ok(UploadResponse.builder()
                .uploadId(context.getUploadId())
                .status("PARTIAL")
                .progress(result.getProgress())
                .message(String.format("Chunk %d/%d uploaded",
                    context.getChunkIndex() + 1, context.getTotalChunks()))
                .build());
        }
    }
}

@Service
public class FileUploadService {

    @Autowired
    private StorageService storageService;

    @Autowired
    private FileMetadataRepository fileMetadataRepository;

    @Autowired
    private ChunkUploadRepository chunkUploadRepository;

    @Value("${app.upload.chunk-size:10485760}") // 10MB
    private long chunkSize;

    @Value("${app.upload.max-file-size:5368709120}") // 5GB
    private long maxFileSize;

    public ChunkUploadResult uploadChunk(UploadContext context) {
        // 1. 验证分片信息
        validateChunkUpload(context);

        // 2. 保存分片到临时存储
        String chunkPath = storageService.saveChunk(
            context.getUploadId(),
            context.getChunkIndex(),
            context.getFile().getInputStream()
        );

        // 3. 记录分片信息
        ChunkInfo chunkInfo = ChunkInfo.builder()
            .uploadId(context.getUploadId())
            .chunkIndex(context.getChunkIndex())
            .chunkSize(context.getFile().getSize())
            .chunkPath(chunkPath)
            .checksum(calculateChecksum(context.getFile()))
            .uploadedAt(LocalDateTime.now())
            .build();

        chunkUploadRepository.save(chunkInfo);

        // 4. 检查是否所有分片都已上传
        long uploadedChunks = chunkUploadRepository.countByUploadId(context.getUploadId());
        boolean isComplete = uploadedChunks == context.getTotalChunks();

        return ChunkUploadResult.builder()
            .uploadId(context.getUploadId())
            .chunkIndex(context.getChunkIndex())
            .isComplete(isComplete)
            .progress((float) uploadedChunks / context.getTotalChunks())
            .build();
    }

    public String mergeChunks(String uploadId) {
        // 1. 获取所有分片信息
        List<ChunkInfo> chunks = chunkUploadRepository.findByUploadIdOrderByChunkIndex(uploadId);

        // 2. 验证分片完整性
        validateChunkIntegrity(chunks);

        // 3. 合并分片
        String mergedFilePath = storageService.mergeChunks(uploadId, chunks);

        // 4. 创建文件元数据
        FileMetadata metadata = FileMetadata.builder()
            .originalName(getOriginalFileName(uploadId))
            .storagePath(mergedFilePath)
            .fileSize(calculateTotalSize(chunks))
            .contentType(detectContentType(mergedFilePath))
            .uploadId(uploadId)
            .status(FileStatus.UPLOADED)
            .uploadedBy(getCurrentUserId())
            .uploadedAt(LocalDateTime.now())
            .build();

        FileMetadata savedMetadata = fileMetadataRepository.save(metadata);

        // 5. 清理临时分片
        cleanupChunks(uploadId);

        return savedMetadata.getId().toString();
    }

    @Async
    public CompletableFuture<String> uploadFromUrl(String url, User user) {
        try {
            // 1. 验证URL
            validateUrl(url);

            // 2. 获取文件信息
            URLConnection connection = new URL(url).openConnection();
            String contentType = connection.getContentType();
            long contentLength = connection.getContentLengthLong();

            // 3. 验证文件大小
            if (contentLength > maxFileSize) {
                throw new FileTooLargeException("File size exceeds maximum allowed size");
            }

            // 4. 下载文件
            String tempPath = downloadFile(url, contentLength);

            // 5. 创建文件元数据
            FileMetadata metadata = FileMetadata.builder()
                .originalName(extractFileNameFromUrl(url))
                .storagePath(tempPath)
                .fileSize(contentLength)
                .contentType(contentType)
                .status(FileStatus.UPLOADED)
                .uploadedBy(user.getId())
                .uploadedAt(LocalDateTime.now())
                .build();

            FileMetadata savedMetadata = fileMetadataRepository.save(metadata);
            return CompletableFuture.completedFuture(savedMetadata.getId().toString());

        } catch (Exception e) {
            logger.error("URL upload failed for: " + url, e);
            throw new CompletionException(e);
        }
    }
}
```

### 动态字段映射系统

**智能字段映射引擎**
```java
@Service
public class FieldMappingService {

    @Autowired
    private FieldMappingRepository mappingRepository;

    @Autowired
    private DataSchemaService schemaService;

    @Autowired
    private MLFieldMappingService mlMappingService;

    public FieldMappingResult analyzeAndMapFields(String fileId) {
        // 1. 数据结构分析
        DataStructureAnalysis analysis = schemaService.analyzeDataStructure(fileId);

        // 2. 自动字段映射推荐
        List<FieldMappingRecommendation> recommendations =
            generateMappingRecommendations(analysis);

        // 3. ML辅助映射
        List<FieldMappingRecommendation> mlRecommendations =
            mlMappingService.recommendMappings(analysis);

        // 4. 合并推荐结果
        List<FieldMappingRecommendation> finalRecommendations =
            mergeRecommendations(recommendations, mlRecommendations);

        return FieldMappingResult.builder()
            .fileId(fileId)
            .analysis(analysis)
            .recommendations(finalRecommendations)
            .confidence(calculateOverallConfidence(finalRecommendations))
            .build();
    }

    private List<FieldMappingRecommendation> generateMappingRecommendations(DataStructureAnalysis analysis) {
        List<FieldMappingRecommendation> recommendations = new ArrayList<>();

        for (FieldInfo field : analysis.getFields()) {
            FieldMappingRecommendation recommendation = new FieldMappingRecommendation();
            recommendation.setSourceField(field.getName());
            recommendation.setSourceType(field.getType());

            // 基于字段名称推荐映射
            TargetField targetField = inferTargetFieldByName(field.getName());
            if (targetField != null) {
                recommendation.setTargetField(targetField.getName());
                recommendation.setTargetType(targetField.getType());
                recommendation.setConfidence(0.8f);
                recommendation.setReason("字段名称匹配");
            } else {
                // 基于数据类型和内容模式推荐
                targetField = inferTargetFieldByContent(field);
                if (targetField != null) {
                    recommendation.setTargetField(targetField.getName());
                    recommendation.setTargetType(targetField.getType());
                    recommendation.setConfidence(0.6f);
                    recommendation.setReason("数据类型和内容模式匹配");
                }
            }

            // 数据转换规则
            if (needsTransformation(field, targetField)) {
                recommendation.setTransformationRule(
                    generateTransformationRule(field, targetField));
            }

            recommendations.add(recommendation);
        }

        return recommendations;
    }

    public FieldMapping createFieldMapping(CreateFieldMappingRequest request) {
        // 1. 验证源字段和目标字段
        validateFieldMapping(request);

        // 2. 创建映射配置
        FieldMapping mapping = FieldMapping.builder()
            .fileId(request.getFileId())
            .sourceField(request.getSourceField())
            .targetField(request.getTargetField())
            .transformationRule(request.getTransformationRule())
            .isRequired(request.isRequired())
            .defaultValue(request.getDefaultValue())
            .validationRules(request.getValidationRules())
            .createdBy(getCurrentUserId())
            .createdAt(LocalDateTime.now())
            .build();

        FieldMapping savedMapping = mappingRepository.save(mapping);

        // 3. 缓存映射配置
        cacheMappingConfiguration(savedMapping);

        return savedMapping;
    }

    public DataTransformationResult applyFieldMappings(String fileId, List<FieldMapping> mappings) {
        try {
            // 1. 读取原始数据
            List<Map<String, Object>> rawData = dataService.readJsonData(fileId);

            // 2. 应用字段映射
            List<Map<String, Object>> transformedData = new ArrayList<>();
            List<TransformationError> errors = new ArrayList<>();

            for (int i = 0; i < rawData.size(); i++) {
                Map<String, Object> rawRecord = rawData.get(i);

                try {
                    Map<String, Object> transformedRecord = applyMappingsToRecord(rawRecord, mappings);
                    transformedData.add(transformedRecord);
                } catch (TransformationException e) {
                    errors.add(new TransformationError(i, e.getMessage(), rawRecord));
                }
            }

            return DataTransformationResult.builder()
                .fileId(fileId)
                .totalRecords(rawData.size())
                .successfulRecords(transformedData.size())
                .errorRecords(errors.size())
                .transformedData(transformedData)
                .errors(errors)
                .build();

        } catch (Exception e) {
            throw new DataTransformationException("Failed to apply field mappings", e);
        }
    }

    private Map<String, Object> applyMappingsToRecord(Map<String, Object> rawRecord,
                                                     List<FieldMapping> mappings) {
        Map<String, Object> transformedRecord = new HashMap<>();

        for (FieldMapping mapping : mappings) {
            try {
                Object sourceValue = extractValue(rawRecord, mapping.getSourceField());
                Object transformedValue = applyTransformation(sourceValue, mapping);

                // 数据验证
                validateValue(transformedValue, mapping);

                transformedRecord.put(mapping.getTargetField(), transformedValue);

            } catch (Exception e) {
                if (mapping.isRequired()) {
                    throw new TransformationException(
                        String.format("Failed to transform required field '%s': %s",
                            mapping.getSourceField(), e.getMessage()));
                } else {
                    // 使用默认值
                    if (mapping.getDefaultValue() != null) {
                        transformedRecord.put(mapping.getTargetField(), mapping.getDefaultValue());
                    }
                }
            }
        }

        return transformedRecord;
    }
}
```

### 数据验证引擎

**多层次数据验证**
```java
@Service
public class DataValidationService {

    @Autowired
    private ValidationRuleRepository ruleRepository;

    @Autowired
    private CustomValidatorRegistry validatorRegistry;

    public ValidationResult validateData(String fileId, List<Map<String, Object>> data) {
        // 1. 获取验证规则
        List<ValidationRule> rules = ruleRepository.findByFileId(fileId);

        // 2. 创建验证上下文
        ValidationContext context = ValidationContext.builder()
            .fileId(fileId)
            .totalRecords(data.size())
            .rules(rules)
            .build();

        // 3. 执行验证
        List<ValidationError> errors = new ArrayList<>();
        int validRecords = 0;

        for (int i = 0; i < data.size(); i++) {
            Map<String, Object> record = data.get(i);
            List<ValidationError> recordErrors = validateRecord(record, rules, i);

            if (recordErrors.isEmpty()) {
                validRecords++;
            } else {
                errors.addAll(recordErrors);
            }
        }

        // 4. 统计结果
        ValidationResult result = ValidationResult.builder()
            .fileId(fileId)
            .totalRecords(data.size())
            .validRecords(validRecords)
            .invalidRecords(data.size() - validRecords)
            .errors(errors)
            .validationRate((float) validRecords / data.size())
            .build();

        return result;
    }

    private List<ValidationError> validateRecord(Map<String, Object> record,
                                               List<ValidationRule> rules, int recordIndex) {
        List<ValidationError> errors = new ArrayList<>();

        for (ValidationRule rule : rules) {
            try {
                validateField(record, rule, recordIndex);
            } catch (FieldValidationException e) {
                errors.add(ValidationError.builder()
                    .recordIndex(recordIndex)
                    .fieldName(rule.getFieldName())
                    .ruleType(rule.getType())
                    .errorMessage(e.getMessage())
                    .actualValue(e.getActualValue())
                    .expectedValue(e.getExpectedValue())
                    .build());
            }
        }

        // 自定义业务规则验证
        errors.addAll(validateBusinessRules(record, recordIndex));

        return errors;
    }

    private void validateField(Map<String, Object> record, ValidationRule rule, int recordIndex) {
        Object value = record.get(rule.getFieldName());

        switch (rule.getType()) {
            case REQUIRED:
                if (value == null || (value instanceof String && ((String) value).trim().isEmpty())) {
                    throw new FieldValidationException(
                        String.format("Field '%s' is required", rule.getFieldName()),
                        null, "non-null value");
                }
                break;

            case DATA_TYPE:
                validateDataType(value, rule.getExpectedType(), rule.getFieldName());
                break;

            case RANGE:
                validateRange(value, rule.getMinValue(), rule.getMaxValue(), rule.getFieldName());
                break;

            case PATTERN:
                validatePattern(value, rule.getPattern(), rule.getFieldName());
                break;

            case ENUM:
                validateEnum(value, rule.getAllowedValues(), rule.getFieldName());
                break;

            case CUSTOM:
                validateCustomRule(record, rule, recordIndex);
                break;
        }
    }

    private void validateDataType(Object value, DataType expectedType, String fieldName) {
        if (value == null) return; // null值由REQUIRED规则处理

        boolean isValid = false;
        switch (expectedType) {
            case STRING:
                isValid = value instanceof String;
                break;
            case INTEGER:
                isValid = value instanceof Integer ||
                         (value instanceof String && isInteger((String) value));
                break;
            case DECIMAL:
                isValid = value instanceof Number ||
                         (value instanceof String && isDecimal((String) value));
                break;
            case BOOLEAN:
                isValid = value instanceof Boolean ||
                         (value instanceof String && isBoolean((String) value));
                break;
            case DATE:
                isValid = value instanceof Date ||
                         (value instanceof String && isDate((String) value));
                break;
            case EMAIL:
                isValid = value instanceof String && isEmail((String) value);
                break;
            case URL:
                isValid = value instanceof String && isUrl((String) value);
                break;
        }

        if (!isValid) {
            throw new FieldValidationException(
                String.format("Field '%s' must be of type %s", fieldName, expectedType),
                value, expectedType.toString());
        }
    }

    private List<ValidationError> validateBusinessRules(Map<String, Object> record, int recordIndex) {
        List<ValidationError> errors = new ArrayList<>();

        // 获取自定义业务验证器
        List<BusinessValidator> validators = validatorRegistry.getValidators();

        for (BusinessValidator validator : validators) {
            try {
                validator.validate(record, recordIndex);
            } catch (BusinessValidationException e) {
                errors.add(ValidationError.builder()
                    .recordIndex(recordIndex)
                    .ruleType(ValidationType.BUSINESS)
                    .errorMessage(e.getMessage())
                    .build());
            }
        }

        return errors;
    }
}

// 自定义业务验证器示例
@Component
public class DocumentBusinessValidator implements BusinessValidator {

    @Override
    public void validate(Map<String, Object> record, int recordIndex) {
        // 业务规则：标题和内容不能同时为空
        String title = (String) record.get("title");
        String content = (String) record.get("content");

        if ((title == null || title.trim().isEmpty()) &&
            (content == null || content.trim().isEmpty())) {
            throw new BusinessValidationException(
                "Document must have either title or content");
        }

        // 业务规则：发布时间不能晚于当前时间
        Object publishTime = record.get("publishTime");
        if (publishTime instanceof String) {
            try {
                LocalDateTime publishDateTime = LocalDateTime.parse((String) publishTime);
                if (publishDateTime.isAfter(LocalDateTime.now())) {
                    throw new BusinessValidationException(
                        "Publish time cannot be in the future");
                }
            } catch (DateTimeParseException e) {
                throw new BusinessValidationException(
                    "Invalid publish time format");
            }
        }
    }
}
```

### 批量导入系统

**高性能批量处理**
```java
@Service
public class BatchImportService {

    @Autowired
    private DocumentRepository documentRepository;

    @Autowired
    private DataValidationService validationService;

    @Autowired
    private VectorService vectorService;

    @Autowired
    private SearchIndexService searchIndexService;

    @Value("${app.batch.size:1000}")
    private int batchSize;

    @Value("${app.batch.thread-pool-size:10}")
    private int threadPoolSize;

    private final ExecutorService executorService =
        Executors.newFixedThreadPool(threadPoolSize);

    @Async
    public CompletableFuture<ImportResult> importData(ImportRequest request) {
        try {
            // 1. 数据预处理
            ImportContext context = prepareImport(request);

            // 2. 分批处理数据
            List<CompletableFuture<BatchResult>> batchFutures = new ArrayList<>();
            List<List<Map<String, Object>>> batches = splitIntoBatches(context.getData(), batchSize);

            for (int i = 0; i < batches.size(); i++) {
                List<Map<String, Object>> batch = batches.get(i);
                int batchIndex = i;

                CompletableFuture<BatchResult> batchFuture = CompletableFuture
                    .supplyAsync(() -> processBatch(batch, batchIndex, context), executorService);

                batchFutures.add(batchFuture);
            }

            // 3. 等待所有批次完成
            CompletableFuture<Void> allBatches = CompletableFuture.allOf(
                batchFutures.toArray(new CompletableFuture[0]));

            return allBatches.thenApply(v -> {
                List<BatchResult> batchResults = batchFutures.stream()
                    .map(CompletableFuture::join)
                    .collect(Collectors.toList());

                return aggregateResults(batchResults, context);
            });

        } catch (Exception e) {
            logger.error("Batch import failed", e);
            return CompletableFuture.completedFuture(
                ImportResult.error("Import failed: " + e.getMessage()));
        }
    }

    private BatchResult processBatch(List<Map<String, Object>> batch, int batchIndex, ImportContext context) {
        try {
            // 1. 数据验证
            ValidationResult validationResult = validationService.validateData(
                context.getFileId(), batch);

            if (validationResult.getValidationRate() < context.getMinValidationRate()) {
                return BatchResult.builder()
                    .batchIndex(batchIndex)
                    .status(BatchStatus.FAILED)
                    .errorMessage("Validation rate too low: " + validationResult.getValidationRate())
                    .build();
            }

            // 2. 数据转换
            List<Document> documents = convertToDocuments(batch, context);

            // 3. 批量保存到数据库
            List<Document> savedDocuments = documentRepository.saveAll(documents);

            // 4. 异步生成向量和索引
            if (context.isGenerateVectors()) {
                generateVectorsAsync(savedDocuments);
            }

            if (context.isCreateSearchIndex()) {
                createSearchIndexAsync(savedDocuments);
            }

            return BatchResult.builder()
                .batchIndex(batchIndex)
                .status(BatchStatus.SUCCESS)
                .processedCount(savedDocuments.size())
                .build();

        } catch (Exception e) {
            logger.error("Batch {} processing failed", batchIndex, e);
            return BatchResult.builder()
                .batchIndex(batchIndex)
                .status(BatchStatus.FAILED)
                .errorMessage(e.getMessage())
                .build();
        }
    }

    private List<Document> convertToDocuments(List<Map<String, Object>> data, ImportContext context) {
        List<Document> documents = new ArrayList<>();

        for (Map<String, Object> record : data) {
            try {
                Document document = Document.builder()
                    .title(extractTitle(record, context))
                    .content(extractContent(record, context))
                    .metadata(extractMetadata(record, context))
                    .channelId(context.getChannelId())
                    .userId(context.getUserId())
                    .status(DocumentStatus.ACTIVE)
                    .createdAt(LocalDateTime.now())
                    .build();

                documents.add(document);
            } catch (Exception e) {
                logger.warn("Failed to convert record to document: {}", e.getMessage());
                // 记录错误但继续处理其他记录
            }
        }

        return documents;
    }

    @Async
    public void generateVectorsAsync(List<Document> documents) {
        try {
            for (Document document : documents) {
                float[] vector = vectorService.generateVector(document.getContent());
                document.setVector(vector);
                documentRepository.save(document);
            }
        } catch (Exception e) {
            logger.error("Vector generation failed", e);
        }
    }

    @Async
    public void createSearchIndexAsync(List<Document> documents) {
        try {
            searchIndexService.indexDocuments(documents);
        } catch (Exception e) {
            logger.error("Search index creation failed", e);
        }
    }

    private ImportResult aggregateResults(List<BatchResult> batchResults, ImportContext context) {
        int totalProcessed = batchResults.stream()
            .mapToInt(BatchResult::getProcessedCount)
            .sum();

        int successfulBatches = (int) batchResults.stream()
            .filter(r -> r.getStatus() == BatchStatus.SUCCESS)
            .count();

        List<String> errors = batchResults.stream()
            .filter(r -> r.getStatus() == BatchStatus.FAILED)
            .map(BatchResult::getErrorMessage)
            .filter(Objects::nonNull)
            .collect(Collectors.toList());

        return ImportResult.builder()
            .fileId(context.getFileId())
            .totalRecords(context.getTotalRecords())
            .processedRecords(totalProcessed)
            .successfulBatches(successfulBatches)
            .totalBatches(batchResults.size())
            .errors(errors)
            .duration(Duration.between(context.getStartTime(), LocalDateTime.now()))
            .build();
    }
}
```

### 数据预览和管理

**数据预览服务**
```java
@RestController
@RequestMapping("/api/data")
public class DataPreviewController {

    @Autowired
    private DataPreviewService previewService;

    @GetMapping("/{fileId}/preview")
    public ResponseEntity<DataPreviewResponse> previewData(
            @PathVariable String fileId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "50") int size,
            @RequestParam(required = false) String filter) {

        try {
            DataPreviewRequest request = DataPreviewRequest.builder()
                .fileId(fileId)
                .page(page)
                .size(size)
                .filter(filter)
                .build();

            DataPreviewResponse response = previewService.getDataPreview(request);
            return ResponseEntity.ok(response);

        } catch (Exception e) {
            logger.error("Data preview failed for file: " + fileId, e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                .body(DataPreviewResponse.error("Preview failed: " + e.getMessage()));
        }
    }

    @GetMapping("/{fileId}/statistics")
    public ResponseEntity<DataStatistics> getDataStatistics(@PathVariable String fileId) {
        try {
            DataStatistics statistics = previewService.calculateStatistics(fileId);
            return ResponseEntity.ok(statistics);
        } catch (Exception e) {
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }
}

@Service
public class DataPreviewService {

    @Autowired
    private DataStorageService storageService;

    private final Cache<String, DataPreviewResponse> previewCache = Caffeine.newBuilder()
        .maximumSize(1000)
        .expireAfterWrite(10, TimeUnit.MINUTES)
        .build();

    public DataPreviewResponse getDataPreview(DataPreviewRequest request) {
        String cacheKey = buildCacheKey(request);

        return previewCache.get(cacheKey, key -> {
            try {
                // 1. 读取数据
                List<Map<String, Object>> allData = storageService.readJsonData(request.getFileId());

                // 2. 应用过滤器
                List<Map<String, Object>> filteredData = applyFilter(allData, request.getFilter());

                // 3. 分页
                List<Map<String, Object>> pagedData = applyPagination(
                    filteredData, request.getPage(), request.getSize());

                // 4. 字段分析
                FieldAnalysis fieldAnalysis = analyzeFields(pagedData);

                return DataPreviewResponse.builder()
                    .fileId(request.getFileId())
                    .totalRecords(allData.size())
                    .filteredRecords(filteredData.size())
                    .data(pagedData)
                    .fieldAnalysis(fieldAnalysis)
                    .page(request.getPage())
                    .size(request.getSize())
                    .build();

            } catch (Exception e) {
                throw new RuntimeException("Failed to generate data preview", e);
            }
        });
    }

    public DataStatistics calculateStatistics(String fileId) {
        try {
            List<Map<String, Object>> data = storageService.readJsonData(fileId);

            DataStatistics statistics = new DataStatistics();
            statistics.setFileId(fileId);
            statistics.setTotalRecords(data.size());

            if (!data.isEmpty()) {
                // 字段统计
                Map<String, FieldStatistics> fieldStats = calculateFieldStatistics(data);
                statistics.setFieldStatistics(fieldStats);

                // 数据质量分析
                DataQualityMetrics qualityMetrics = analyzeDataQuality(data);
                statistics.setQualityMetrics(qualityMetrics);
            }

            return statistics;

        } catch (Exception e) {
            throw new RuntimeException("Failed to calculate statistics", e);
        }
    }

    private Map<String, FieldStatistics> calculateFieldStatistics(List<Map<String, Object>> data) {
        Map<String, FieldStatistics> fieldStats = new HashMap<>();

        // 获取所有字段名
        Set<String> allFields = data.stream()
            .flatMap(record -> record.keySet().stream())
            .collect(Collectors.toSet());

        for (String fieldName : allFields) {
            FieldStatistics stats = new FieldStatistics();
            stats.setFieldName(fieldName);

            List<Object> values = data.stream()
                .map(record -> record.get(fieldName))
                .collect(Collectors.toList());

            // 统计非空值
            long nonNullCount = values.stream()
                .filter(Objects::nonNull)
                .count();

            stats.setTotalCount(values.size());
            stats.setNonNullCount(nonNullCount);
            stats.setNullCount(values.size() - nonNullCount);
            stats.setNullRate((double) stats.getNullCount() / values.size());

            // 数据类型分析
            Map<String, Long> typeDistribution = values.stream()
                .filter(Objects::nonNull)
                .collect(Collectors.groupingBy(
                    value -> value.getClass().getSimpleName(),
                    Collectors.counting()));
            stats.setTypeDistribution(typeDistribution);

            // 唯一值统计
            long uniqueCount = values.stream()
                .filter(Objects::nonNull)
                .distinct()
                .count();
            stats.setUniqueCount(uniqueCount);

            fieldStats.put(fieldName, stats);
        }

        return fieldStats;
    }
}
```

## 数据库设计

```sql
-- 文件元数据表
CREATE TABLE file_metadata (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    original_name VARCHAR(255) NOT NULL,
    storage_path VARCHAR(500) NOT NULL,
    file_size BIGINT NOT NULL,
    content_type VARCHAR(100),
    upload_id VARCHAR(100) UNIQUE,
    status ENUM('UPLOADING', 'UPLOADED', 'PROCESSING', 'PROCESSED', 'ERROR') DEFAULT 'UPLOADING',
    uploaded_by BIGINT NOT NULL,
    uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed_at TIMESTAMP NULL,
    FOREIGN KEY (uploaded_by) REFERENCES users(id),
    INDEX idx_upload_id (upload_id),
    INDEX idx_status (status),
    INDEX idx_uploaded_by (uploaded_by)
);

-- 分片上传信息表
CREATE TABLE chunk_uploads (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    upload_id VARCHAR(100) NOT NULL,
    chunk_index INT NOT NULL,
    chunk_size BIGINT NOT NULL,
    chunk_path VARCHAR(500) NOT NULL,
    checksum VARCHAR(64),
    uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE KEY uk_upload_chunk (upload_id, chunk_index),
    INDEX idx_upload_id (upload_id)
);

-- 字段映射配置表
CREATE TABLE field_mappings (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    file_id VARCHAR(100) NOT NULL,
    source_field VARCHAR(200) NOT NULL,
    target_field VARCHAR(200) NOT NULL,
    transformation_rule JSON,
    is_required BOOLEAN DEFAULT FALSE,
    default_value VARCHAR(500),
    validation_rules JSON,
    created_by BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (created_by) REFERENCES users(id),
    UNIQUE KEY uk_file_source_target (file_id, source_field, target_field),
    INDEX idx_file_id (file_id)
);

-- 数据验证规则表
CREATE TABLE validation_rules (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    file_id VARCHAR(100) NOT NULL,
    field_name VARCHAR(200) NOT NULL,
    rule_type ENUM('REQUIRED', 'DATA_TYPE', 'RANGE', 'PATTERN', 'ENUM', 'CUSTOM') NOT NULL,
    expected_type VARCHAR(50),
    min_value VARCHAR(100),
    max_value VARCHAR(100),
    pattern VARCHAR(500),
    allowed_values JSON,
    custom_validator VARCHAR(200),
    error_message VARCHAR(500),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_file_field (file_id, field_name),
    INDEX idx_rule_type (rule_type)
);

-- 数据导入任务表
CREATE TABLE import_tasks (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    file_id VARCHAR(100) NOT NULL,
    channel_id BIGINT NOT NULL,
    total_records INT DEFAULT 0,
    processed_records INT DEFAULT 0,
    successful_records INT DEFAULT 0,
    failed_records INT DEFAULT 0,
    status ENUM('PENDING', 'PROCESSING', 'COMPLETED', 'FAILED', 'CANCELLED') DEFAULT 'PENDING',
    error_message TEXT,
    started_at TIMESTAMP NULL,
    completed_at TIMESTAMP NULL,
    created_by BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (channel_id) REFERENCES channels(id),
    FOREIGN KEY (created_by) REFERENCES users(id),
    INDEX idx_file_id (file_id),
    INDEX idx_status (status),
    INDEX idx_created_by (created_by)
);

-- 数据导入错误日志表
CREATE TABLE import_errors (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    task_id BIGINT NOT NULL,
    record_index INT NOT NULL,
    field_name VARCHAR(200),
    error_type VARCHAR(100) NOT NULL,
    error_message TEXT NOT NULL,
    record_data JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (task_id) REFERENCES import_tasks(id),
    INDEX idx_task_id (task_id),
    INDEX idx_error_type (error_type)
);
```

## 依赖关系

**前置依赖**:
- 005: 三层权限管理系统

**后续任务依赖此任务**:
- 无 (数据管理系统为支撑功能)

## 工作量估算

**规模**: M (中型)
**预估时间**: 2周
**并行性**: false (依赖权限系统)

### 分解任务
1. 文件上传系统开发 (3天)
2. 动态字段映射引擎 (3天)
3. 数据验证框架 (2天)
4. 批量导入处理 (3天)
5. 数据预览和统计 (2天)
6. 错误处理和日志 (1天)
7. 单元测试和集成测试 (2天)

## 完成定义

任务完成需满足以下条件：
1. 所有验收标准的性能指标达标
2. 文件上传稳定性测试通过
3. 数据验证准确性验证完成
4. 批量导入性能测试达标
5. 错误处理机制验证通过
6. 代码覆盖率达到85%以上

## 风险与缓解

**技术风险**:
- 大文件处理内存溢出 → 流式处理，分片上传
- 数据验证性能瓶颈 → 并行验证，规则优化
- 字段映射复杂度过高 → 简化配置，提供模板

**业务风险**:
- 数据质量问题 → 多层验证，人工审核机制
- 导入失败回滚复杂 → 事务管理，检查点机制

**性能风险**:
- 批量导入阻塞系统 → 异步处理，资源隔离
- 并发上传冲突 → 文件锁机制，队列管理
