---
id: 008
title: 统计分析和系统完善
epic: 创建智能搜索平台
status: pending
priority: medium
assignee: ""
labels: ["统计分析", "定时任务", "系统测试", "性能优化", "部署配置"]
created: 2025-09-20T02:23:58Z
updated: 2025-09-20T02:45:33Z
estimate: M
parallel: false
dependencies: [004, 005, 006, 007]
---

# 统计分析和系统完善

## 描述

开发统计分析模块，实现搜索数据的收集、分析和可视化。包括搜索热词统计、点击率分析、零结果率监控等核心指标计算。同时完善系统的定时任务、性能优化、系统测试和Docker容器化部署配置，确保系统稳定运行和高性能表现。

## 验收标准

- [ ] 统计数据实时更新延迟 < 5分钟
- [ ] 搜索热词TOP100计算准确率 > 99%
- [ ] 点击率统计误差 < 1%
- [ ] 零结果率监控实时告警
- [ ] 系统性能测试支持200并发用户
- [ ] 响应时间P95 < 500ms，P99 < 1秒
- [ ] 数据备份和恢复完整性验证通过
- [ ] Docker容器启动时间 < 30秒
- [ ] 系统可用性 > 99.9% (月度统计)
- [ ] 内存使用率稳定在80%以下
- [ ] CPU使用率平均值 < 70%
- [ ] 定时任务执行成功率 > 99%

## 技术细节

### 统计分析模块

**搜索统计数据模型**
```java
@Entity
@Table(name = "search_statistics")
public class SearchStatistics {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "date", nullable = false)
    @Temporal(TemporalType.DATE)
    private Date date;

    @Column(name = "hour", nullable = false)
    private Integer hour;

    @Column(name = "total_searches", nullable = false)
    private Long totalSearches = 0L;

    @Column(name = "unique_users", nullable = false)
    private Long uniqueUsers = 0L;

    @Column(name = "zero_result_searches", nullable = false)
    private Long zeroResultSearches = 0L;

    @Column(name = "avg_response_time", nullable = false)
    private Double avgResponseTime = 0.0;

    @Column(name = "total_clicks", nullable = false)
    private Long totalClicks = 0L;

    @Column(name = "created_at", nullable = false)
    @Temporal(TemporalType.TIMESTAMP)
    private Date createdAt;

    @Column(name = "updated_at", nullable = false)
    @Temporal(TemporalType.TIMESTAMP)
    private Date updatedAt;

    // 构造函数、getter、setter
    public SearchStatistics() {
        this.createdAt = new Date();
        this.updatedAt = new Date();
    }

    @PreUpdate
    public void preUpdate() {
        this.updatedAt = new Date();
    }

    // 计算零结果率
    public Double getZeroResultRate() {
        return totalSearches > 0 ? (double) zeroResultSearches / totalSearches : 0.0;
    }

    // 计算点击率
    public Double getClickThroughRate() {
        return totalSearches > 0 ? (double) totalClicks / totalSearches : 0.0;
    }
}
```

**搜索热词统计**
```java
@Entity
@Table(name = "search_keywords",
       indexes = {
           @Index(name = "idx_keyword_date", columnList = "keyword, date"),
           @Index(name = "idx_search_count", columnList = "search_count DESC")
       })
public class SearchKeyword {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "keyword", nullable = false, length = 200)
    private String keyword;

    @Column(name = "search_count", nullable = false)
    private Long searchCount = 0L;

    @Column(name = "click_count", nullable = false)
    private Long clickCount = 0L;

    @Column(name = "zero_result_count", nullable = false)
    private Long zeroResultCount = 0L;

    @Column(name = "date", nullable = false)
    @Temporal(TemporalType.DATE)
    private Date date;

    @Column(name = "avg_position_clicked", nullable = false)
    private Double avgPositionClicked = 0.0;

    @Column(name = "unique_users", nullable = false)
    private Long uniqueUsers = 0L;

    @Column(name = "created_at", nullable = false)
    @Temporal(TemporalType.TIMESTAMP)
    private Date createdAt;

    @Column(name = "updated_at", nullable = false)
    @Temporal(TemporalType.TIMESTAMP)
    private Date updatedAt;

    public SearchKeyword() {
        this.createdAt = new Date();
        this.updatedAt = new Date();
    }

    @PreUpdate
    public void preUpdate() {
        this.updatedAt = new Date();
    }

    // 计算点击率
    public Double getClickThroughRate() {
        return searchCount > 0 ? (double) clickCount / searchCount : 0.0;
    }

    // 计算零结果率
    public Double getZeroResultRate() {
        return searchCount > 0 ? (double) zeroResultCount / searchCount : 0.0;
    }
}
```

**统计分析服务**
```java
@Service
@Transactional
@Slf4j
public class AnalyticsService {

    @Autowired
    private SearchStatisticsRepository statisticsRepository;

    @Autowired
    private SearchKeywordRepository keywordRepository;

    @Autowired
    private SearchLogRepository searchLogRepository;

    @Autowired
    private RedisTemplate<String, Object> redisTemplate;

    @Autowired
    private KafkaTemplate<String, Object> kafkaTemplate;

    private static final String ANALYTICS_CACHE_PREFIX = "analytics:";
    private static final String HOT_KEYWORDS_KEY = "hot_keywords:";

    /**
     * 实时更新搜索统计
     */
    @Async
    public void updateSearchStatistics(SearchEvent event) {
        try {
            Date now = new Date();
            String dateKey = DateUtil.formatDate(now, "yyyy-MM-dd");
            int hour = DateUtil.getHour(now);

            // 更新小时级统计
            SearchStatistics hourlyStats = statisticsRepository
                .findByDateAndHour(DateUtil.parseDate(dateKey, "yyyy-MM-dd"), hour)
                .orElse(new SearchStatistics());

            hourlyStats.setDate(DateUtil.parseDate(dateKey, "yyyy-MM-dd"));
            hourlyStats.setHour(hour);
            hourlyStats.setTotalSearches(hourlyStats.getTotalSearches() + 1);

            // 更新响应时间
            updateResponseTime(hourlyStats, event.getResponseTime());

            // 检查是否为零结果搜索
            if (event.getResultCount() == 0) {
                hourlyStats.setZeroResultSearches(hourlyStats.getZeroResultSearches() + 1);
            }

            // 更新用户数（使用HyperLogLog估算）
            String userCountKey = ANALYTICS_CACHE_PREFIX + "users:" + dateKey + ":" + hour;
            redisTemplate.opsForHyperLogLog().add(userCountKey, event.getUserId());
            Long uniqueUsers = redisTemplate.opsForHyperLogLog().size(userCountKey);
            hourlyStats.setUniqueUsers(uniqueUsers);

            statisticsRepository.save(hourlyStats);

            // 更新关键词统计
            updateKeywordStatistics(event);

            // 发送实时分析事件
            kafkaTemplate.send("analytics-events", event);

            log.debug("搜索统计已更新: {}", event.getQuery());

        } catch (Exception e) {
            log.error("更新搜索统计失败", e);
        }
    }

    /**
     * 更新关键词统计
     */
    private void updateKeywordStatistics(SearchEvent event) {
        String keyword = event.getQuery().toLowerCase().trim();
        String dateKey = DateUtil.formatDate(new Date(), "yyyy-MM-dd");

        SearchKeyword keywordStats = keywordRepository
            .findByKeywordAndDate(keyword, DateUtil.parseDate(dateKey, "yyyy-MM-dd"))
            .orElse(new SearchKeyword());

        keywordStats.setKeyword(keyword);
        keywordStats.setDate(DateUtil.parseDate(dateKey, "yyyy-MM-dd"));
        keywordStats.setSearchCount(keywordStats.getSearchCount() + 1);

        if (event.getResultCount() == 0) {
            keywordStats.setZeroResultCount(keywordStats.getZeroResultCount() + 1);
        }

        // 更新用户数
        String userKey = ANALYTICS_CACHE_PREFIX + "keyword_users:" + keyword + ":" + dateKey;
        redisTemplate.opsForHyperLogLog().add(userKey, event.getUserId());
        Long uniqueUsers = redisTemplate.opsForHyperLogLog().size(userKey);
        keywordStats.setUniqueUsers(uniqueUsers);

        keywordRepository.save(keywordStats);

        // 更新热词缓存
        updateHotKeywordsCache(keyword, keywordStats.getSearchCount());
    }

    /**
     * 更新点击统计
     */
    @Async
    public void updateClickStatistics(ClickEvent event) {
        try {
            String dateKey = DateUtil.formatDate(new Date(), "yyyy-MM-dd");
            int hour = DateUtil.getHour(new Date());

            // 更新小时级点击统计
            SearchStatistics hourlyStats = statisticsRepository
                .findByDateAndHour(DateUtil.parseDate(dateKey, "yyyy-MM-dd"), hour)
                .orElse(new SearchStatistics());

            hourlyStats.setTotalClicks(hourlyStats.getTotalClicks() + 1);
            statisticsRepository.save(hourlyStats);

            // 更新关键词点击统计
            updateKeywordClickStatistics(event);

        } catch (Exception e) {
            log.error("更新点击统计失败", e);
        }
    }

    /**
     * 更新关键词点击统计
     */
    private void updateKeywordClickStatistics(ClickEvent event) {
        String keyword = event.getQuery().toLowerCase().trim();
        String dateKey = DateUtil.formatDate(new Date(), "yyyy-MM-dd");

        SearchKeyword keywordStats = keywordRepository
            .findByKeywordAndDate(keyword, DateUtil.parseDate(dateKey, "yyyy-MM-dd"))
            .orElse(null);

        if (keywordStats != null) {
            keywordStats.setClickCount(keywordStats.getClickCount() + 1);

            // 更新平均点击位置
            updateAverageClickPosition(keywordStats, event.getPosition());

            keywordRepository.save(keywordStats);
        }
    }

    /**
     * 获取热门搜索词
     */
    @Cacheable(value = "hotKeywords", key = "#days + '_' + #limit")
    public List<KeywordStatsDTO> getHotKeywords(int days, int limit) {
        Date startDate = DateUtil.addDays(new Date(), -days);
        Date endDate = new Date();

        List<Object[]> results = keywordRepository.findHotKeywords(startDate, endDate, limit);

        return results.stream()
            .map(result -> KeywordStatsDTO.builder()
                .keyword((String) result[0])
                .searchCount((Long) result[1])
                .clickCount((Long) result[2])
                .zeroResultCount((Long) result[3])
                .uniqueUsers((Long) result[4])
                .clickThroughRate(calculateClickThroughRate((Long) result[1], (Long) result[2]))
                .zeroResultRate(calculateZeroResultRate((Long) result[1], (Long) result[3]))
                .build())
            .collect(Collectors.toList());
    }

    /**
     * 获取搜索趋势数据
     */
    public List<SearchTrendDTO> getSearchTrend(String period, int days) {
        Date startDate = DateUtil.addDays(new Date(), -days);
        Date endDate = new Date();

        List<Object[]> results;
        if ("hour".equals(period)) {
            results = statisticsRepository.findHourlyTrend(startDate, endDate);
        } else {
            results = statisticsRepository.findDailyTrend(startDate, endDate);
        }

        return results.stream()
            .map(result -> SearchTrendDTO.builder()
                .timestamp((Date) result[0])
                .totalSearches((Long) result[1])
                .uniqueUsers((Long) result[2])
                .zeroResultRate((Double) result[3])
                .avgResponseTime((Double) result[4])
                .totalClicks((Long) result[5])
                .clickThroughRate(calculateClickThroughRate((Long) result[1], (Long) result[5]))
                .build())
            .collect(Collectors.toList());
    }

    /**
     * 实时监控零结果率
     */
    @Scheduled(fixedRate = 300000) // 每5分钟检查一次
    public void monitorZeroResultRate() {
        try {
            Date now = new Date();
            Date oneHourAgo = DateUtil.addHours(now, -1);

            Double zeroResultRate = statisticsRepository.calculateZeroResultRate(oneHourAgo, now);

            // 设置告警阈值
            double alertThreshold = 0.20; // 20%

            if (zeroResultRate > alertThreshold) {
                // 发送告警
                AlertEvent alertEvent = AlertEvent.builder()
                    .type("ZERO_RESULT_RATE_HIGH")
                    .message(String.format("零结果率过高: %.2f%%, 阈值: %.2f%%",
                        zeroResultRate * 100, alertThreshold * 100))
                    .value(zeroResultRate)
                    .threshold(alertThreshold)
                    .timestamp(now)
                    .build();

                kafkaTemplate.send("alerts", alertEvent);

                log.warn("零结果率告警: 当前值 {}%, 阈值 {}%",
                    zeroResultRate * 100, alertThreshold * 100);
            }

        } catch (Exception e) {
            log.error("监控零结果率失败", e);
        }
    }

    /**
     * 更新热词缓存
     */
    private void updateHotKeywordsCache(String keyword, Long searchCount) {
        String cacheKey = HOT_KEYWORDS_KEY + DateUtil.formatDate(new Date(), "yyyy-MM-dd");
        redisTemplate.opsForZSet().add(cacheKey, keyword, searchCount);

        // 设置过期时间为7天
        redisTemplate.expire(cacheKey, 7, TimeUnit.DAYS);
    }

    /**
     * 更新平均响应时间
     */
    private void updateResponseTime(SearchStatistics stats, Long responseTime) {
        Long currentTotal = stats.getTotalSearches();
        Double currentAvg = stats.getAvgResponseTime();

        // 计算新的平均值
        Double newAvg = (currentAvg * currentTotal + responseTime) / (currentTotal + 1);
        stats.setAvgResponseTime(newAvg);
    }

    /**
     * 更新平均点击位置
     */
    private void updateAverageClickPosition(SearchKeyword stats, Integer position) {
        Long clickCount = stats.getClickCount();
        Double currentAvg = stats.getAvgPositionClicked();

        // 计算新的平均位置
        Double newAvg = (currentAvg * (clickCount - 1) + position) / clickCount;
        stats.setAvgPositionClicked(newAvg);
    }

    /**
     * 计算点击率
     */
    private Double calculateClickThroughRate(Long searches, Long clicks) {
        return searches > 0 ? (double) clicks / searches : 0.0;
    }

    /**
     * 计算零结果率
     */
    private Double calculateZeroResultRate(Long searches, Long zeroResults) {
        return searches > 0 ? (double) zeroResults / searches : 0.0;
    }
}
```

### 定时任务系统

**定时任务配置**
```java
@Configuration
@EnableScheduling
@EnableAsync
@Slf4j
public class ScheduledTaskConfig {

    @Autowired
    private AnalyticsService analyticsService;

    @Autowired
    private DataBackupService dataBackupService;

    @Autowired
    private SystemMaintenanceService maintenanceService;

    /**
     * 每日数据汇总
     */
    @Scheduled(cron = "0 5 1 * * ?") // 每天凌晨1:05执行
    public void dailyDataAggregation() {
        log.info("开始执行每日数据汇总任务");
        try {
            analyticsService.aggregateDailyData();
            log.info("每日数据汇总任务完成");
        } catch (Exception e) {
            log.error("每日数据汇总任务失败", e);
        }
    }

    /**
     * 数据备份
     */
    @Scheduled(cron = "0 0 2 * * ?") // 每天凌晨2:00执行
    public void dataBackup() {
        log.info("开始执行数据备份任务");
        try {
            dataBackupService.performBackup();
            log.info("数据备份任务完成");
        } catch (Exception e) {
            log.error("数据备份任务失败", e);
        }
    }

    /**
     * 清理过期数据
     */
    @Scheduled(cron = "0 30 3 * * ?") // 每天凌晨3:30执行
    public void cleanupExpiredData() {
        log.info("开始执行过期数据清理任务");
        try {
            maintenanceService.cleanupExpiredData();
            log.info("过期数据清理任务完成");
        } catch (Exception e) {
            log.error("过期数据清理任务失败", e);
        }
    }

    /**
     * 系统健康检查
     */
    @Scheduled(fixedRate = 300000) // 每5分钟执行
    public void systemHealthCheck() {
        try {
            maintenanceService.performHealthCheck();
        } catch (Exception e) {
            log.error("系统健康检查失败", e);
        }
    }

    /**
     * 缓存预热
     */
    @Scheduled(cron = "0 0 4 * * ?") // 每天凌晨4:00执行
    public void cacheWarmup() {
        log.info("开始执行缓存预热任务");
        try {
            maintenanceService.warmupCache();
            log.info("缓存预热任务完成");
        } catch (Exception e) {
            log.error("缓存预热任务失败", e);
        }
    }
}
```

### 性能优化

**数据库优化配置**
```yaml
# application.yml
spring:
  datasource:
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      idle-timeout: 300000
      connection-timeout: 20000
      validation-timeout: 5000
      leak-detection-threshold: 60000

  jpa:
    hibernate:
      ddl-auto: none
    properties:
      hibernate:
        jdbc:
          batch_size: 50
          order_inserts: true
          order_updates: true
        cache:
          use_second_level_cache: true
          use_query_cache: true
          region:
            factory_class: org.hibernate.cache.ehcache.EhCacheRegionFactory

  redis:
    jedis:
      pool:
        max-active: 20
        max-idle: 10
        min-idle: 5
        max-wait: 2000ms
    timeout: 5000ms

# 性能监控配置
management:
  endpoints:
    web:
      exposure:
        include: health,metrics,prometheus
  endpoint:
    health:
      show-details: always
  metrics:
    export:
      prometheus:
        enabled: true
```

**缓存优化策略**
```java
@Service
@Slf4j
public class CacheOptimizationService {

    @Autowired
    private RedisTemplate<String, Object> redisTemplate;

    private static final String CACHE_PREFIX = "search_cache:";
    private static final int DEFAULT_TTL = 3600; // 1小时

    /**
     * 多级缓存策略
     */
    @Cacheable(value = "searchResults", key = "#query + '_' + #page",
               unless = "#result == null || #result.isEmpty()")
    public List<SearchResult> getCachedSearchResults(String query, int page) {
        String cacheKey = CACHE_PREFIX + "results:" + DigestUtils.md5Hex(query) + ":" + page;

        try {
            // L1缓存：本地缓存 (已通过@Cacheable实现)
            // L2缓存：Redis缓存
            Object cached = redisTemplate.opsForValue().get(cacheKey);
            if (cached != null) {
                return (List<SearchResult>) cached;
            }

            return null;
        } catch (Exception e) {
            log.warn("获取缓存失败: {}", cacheKey, e);
            return null;
        }
    }

    /**
     * 智能缓存预热
     */
    @Async
    public void preloadPopularQueries() {
        try {
            // 获取热门搜索词
            List<String> hotQueries = getHotQueries(100);

            for (String query : hotQueries) {
                // 预加载前3页结果
                for (int page = 1; page <= 3; page++) {
                    String cacheKey = CACHE_PREFIX + "results:" + DigestUtils.md5Hex(query) + ":" + page;

                    if (!redisTemplate.hasKey(cacheKey)) {
                        // 模拟搜索请求进行预热
                        preloadSearchResults(query, page);
                    }
                }
            }

            log.info("缓存预热完成，预热查询数: {}", hotQueries.size());
        } catch (Exception e) {
            log.error("缓存预热失败", e);
        }
    }

    /**
     * 缓存失效策略
     */
    @EventListener
    public void handleDataUpdate(DataUpdateEvent event) {
        try {
            // 根据更新的数据类型，精确失效相关缓存
            String pattern = CACHE_PREFIX + "*";
            if (event.getDocumentId() != null) {
                pattern = CACHE_PREFIX + "*doc_" + event.getDocumentId() + "*";
            }

            Set<String> keys = redisTemplate.keys(pattern);
            if (!keys.isEmpty()) {
                redisTemplate.delete(keys);
                log.info("缓存失效完成，失效key数量: {}", keys.size());
            }
        } catch (Exception e) {
            log.error("缓存失效失败", e);
        }
    }

    private List<String> getHotQueries(int limit) {
        String hotKeywordsKey = "hot_keywords:" + DateUtil.formatDate(new Date(), "yyyy-MM-dd");
        Set<Object> hotKeywords = redisTemplate.opsForZSet()
            .reverseRange(hotKeywordsKey, 0, limit - 1);

        return hotKeywords.stream()
            .map(Object::toString)
            .collect(Collectors.toList());
    }

    private void preloadSearchResults(String query, int page) {
        // 实际的搜索逻辑会触发缓存存储
        // 这里仅作示例
        log.debug("预热缓存: query={}, page={}", query, page);
    }
}
```

### 系统测试

**性能测试脚本**
```java
@Test
public class PerformanceTest {

    @Autowired
    private TestRestTemplate restTemplate;

    @Test
    public void testConcurrentSearchPerformance() throws InterruptedException {
        int threadCount = 200;
        int requestsPerThread = 10;
        CountDownLatch latch = new CountDownLatch(threadCount);
        List<Long> responseTimes = Collections.synchronizedList(new ArrayList<>());
        AtomicInteger successCount = new AtomicInteger(0);
        AtomicInteger errorCount = new AtomicInteger(0);

        ExecutorService executor = Executors.newFixedThreadPool(threadCount);

        for (int i = 0; i < threadCount; i++) {
            final int threadIndex = i;
            executor.submit(() -> {
                try {
                    for (int j = 0; j < requestsPerThread; j++) {
                        long startTime = System.currentTimeMillis();

                        ResponseEntity<String> response = restTemplate.getForEntity(
                            "/api/search?q=test" + (threadIndex % 10), String.class);

                        long responseTime = System.currentTimeMillis() - startTime;
                        responseTimes.add(responseTime);

                        if (response.getStatusCode().is2xxSuccessful()) {
                            successCount.incrementAndGet();
                        } else {
                            errorCount.incrementAndGet();
                        }
                    }
                } catch (Exception e) {
                    errorCount.incrementAndGet();
                    log.error("请求失败", e);
                } finally {
                    latch.countDown();
                }
            });
        }

        latch.await(120, TimeUnit.SECONDS); // 最多等待2分钟
        executor.shutdown();

        // 分析结果
        responseTimes.sort(Long::compareTo);
        long p95 = responseTimes.get((int) (responseTimes.size() * 0.95));
        long p99 = responseTimes.get((int) (responseTimes.size() * 0.99));
        double avgResponseTime = responseTimes.stream().mapToLong(Long::longValue).average().orElse(0);

        log.info("性能测试结果:");
        log.info("总请求数: {}", threadCount * requestsPerThread);
        log.info("成功请求数: {}", successCount.get());
        log.info("失败请求数: {}", errorCount.get());
        log.info("平均响应时间: {}ms", avgResponseTime);
        log.info("P95响应时间: {}ms", p95);
        log.info("P99响应时间: {}ms", p99);

        // 验收标准验证
        assertThat(p95).isLessThan(500); // P95 < 500ms
        assertThat(p99).isLessThan(1000); // P99 < 1s
        assertThat(successCount.get()).isGreaterThan(threadCount * requestsPerThread * 0.99); // 成功率 > 99%
    }

    @Test
    public void testMemoryUsage() {
        MemoryMXBean memoryBean = ManagementFactory.getMemoryMXBean();
        MemoryUsage heapUsage = memoryBean.getHeapMemoryUsage();

        long maxHeap = heapUsage.getMax();
        long usedHeap = heapUsage.getUsed();
        double usagePercentage = (double) usedHeap / maxHeap * 100;

        log.info("堆内存使用情况:");
        log.info("最大堆内存: {}MB", maxHeap / 1024 / 1024);
        log.info("已使用堆内存: {}MB", usedHeap / 1024 / 1024);
        log.info("内存使用率: {:.2f}%", usagePercentage);

        // 验收标准验证
        assertThat(usagePercentage).isLessThan(80.0); // 内存使用率 < 80%
    }
}
```

### Docker部署配置

**Docker Compose配置**
```yaml
# docker-compose.yml
version: '3.8'

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=production
      - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/deep_search
      - SPRING_REDIS_HOST=redis
      - ELASTICSEARCH_HOST=elasticsearch
    depends_on:
      - mysql
      - redis
      - elasticsearch
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  frontend:
    build:
      context: ./frontend
      dockerfile: frontend.Dockerfile
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped

  mysql:
    image: mysql:8.0
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=rootpassword
      - MYSQL_DATABASE=deep_search
      - MYSQL_USER=app
      - MYSQL_PASSWORD=apppassword
    volumes:
      - mysql_data:/var/lib/mysql
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped
    command: --default-authentication-plugin=mysql_native_password

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped

volumes:
  mysql_data:
  redis_data:
  elasticsearch_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

**应用Dockerfile**
```dockerfile
# Dockerfile
FROM openjdk:17-jdk-slim AS builder

WORKDIR /app
COPY gradlew .
COPY gradle gradle
COPY build.gradle .
COPY settings.gradle .
COPY src src

# 构建应用
RUN chmod +x gradlew && ./gradlew bootJar

FROM openjdk:17-jre-slim

# 安装必要工具
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# 复制构建的jar文件
COPY --from=builder /app/build/libs/*.jar app.jar

# 创建日志目录
RUN mkdir -p /app/logs

# 设置JVM参数
ENV JAVA_OPTS="-Xms1g -Xmx2g -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/app/logs/"

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8080/actuator/health || exit 1

EXPOSE 8080

ENTRYPOINT exec java $JAVA_OPTS -jar app.jar
```

### 监控配置

**Prometheus配置**
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'deep-search-backend'
    static_configs:
      - targets: ['backend:8080']
    metrics_path: '/actuator/prometheus'
    scrape_interval: 15s

  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql:9104']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:9121']

  - job_name: 'elasticsearch'
    static_configs:
      - targets: ['elasticsearch:9200']
```

**Grafana仪表板配置**
```json
{
  "dashboard": {
    "title": "智能搜索平台监控",
    "panels": [
      {
        "title": "搜索QPS",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(search_requests_total[5m])",
            "legendFormat": "搜索请求/秒"
          }
        ]
      },
      {
        "title": "响应时间分布",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, search_duration_seconds_bucket)",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, search_duration_seconds_bucket)",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "title": "系统资源使用",
        "type": "graph",
        "targets": [
          {
            "expr": "process_memory_usage_bytes / 1024 / 1024",
            "legendFormat": "内存使用(MB)"
          },
          {
            "expr": "process_cpu_usage * 100",
            "legendFormat": "CPU使用率(%)"
          }
        ]
      }
    ]
  }
}
```

## 部署流程

### 生产环境部署脚本
```bash
#!/bin/bash
# deploy.sh

set -e

echo "开始部署智能搜索平台..."

# 1. 构建镜像
echo "构建Docker镜像..."
docker-compose build

# 2. 停止旧服务
echo "停止旧服务..."
docker-compose down

# 3. 备份数据
echo "备份数据库..."
docker exec mysql_container mysqldump -u root -p deep_search > backup_$(date +%Y%m%d_%H%M%S).sql

# 4. 启动新服务
echo "启动新服务..."
docker-compose up -d

# 5. 等待服务启动
echo "等待服务启动..."
sleep 30

# 6. 健康检查
echo "执行健康检查..."
curl -f http://localhost:8080/actuator/health || (echo "健康检查失败" && exit 1)

# 7. 运行烟雾测试
echo "执行烟雾测试..."
curl -f "http://localhost:8080/api/search?q=test" || (echo "烟雾测试失败" && exit 1)

echo "部署完成！"
echo "访问地址: http://localhost"
echo "管理后台: http://localhost/admin"
echo "监控面板: http://localhost:3000"
```

## 验收测试

### 自动化测试脚本
```bash
#!/bin/bash
# acceptance_test.sh

echo "执行验收测试..."

# 性能测试
echo "1. 执行性能测试..."
./gradlew test --tests PerformanceTest

# 功能测试
echo "2. 执行功能测试..."
./gradlew test --tests IntegrationTest

# 统计准确性测试
echo "3. 执行统计准确性测试..."
./gradlew test --tests AnalyticsAccuracyTest

# 系统可用性测试
echo "4. 执行系统可用性测试..."
./scripts/availability_test.sh

echo "验收测试完成！"
```
